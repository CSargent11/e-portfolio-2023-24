<!DOCTYPE html>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<style>
    .centered {
        display: flex;
        justify-content: center;
        align-items: center;
    }
</style>
	<head>
		<li><a href="index.html"><span class="icon solid fa-home">Home</span></a></li>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		</div>

		<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
.collapsible {
  background-color: sage;
  color: grey;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #1E90FF;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #B0C4DE;
}
</style>
</head>

<!-- Main -->

		<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">
	<div>
	
	<h2>Research Methods and professional practice</br>
		Module 7</h2>

		</br>
	<button type="button" class="collapsible">Unit 1</button>
<div class="content">
  <p><b>Unit 1: Introduction to Research Methods. The Scientific Investigation and Ethics in Computing</b>
</br>
	<style>
	.fit-page {
  max-width: 100%;
  height: auto;}
	</style>
	<head>
    <i>Discussion post 1:</i>
</head>
	<body>
    <img src="images/U1.1.png" alt="Initial Post"class="fit-page">
	<img src="images/U1.2.png" alt="Initial Post"class="fit-page">
	<img src="images/U1.3.png" alt="Initial Post"class="fit-page">

	<b>Reflective Activity 1: Ethics in Computing</b></br>
		</br>
<i>Read Correa et al. (2023) and Deckard (2023).</br>
Reflection on AI Governance: Legal, Social, and Professional Implications</i></br>
</br>
		Introduction</br>
		Generative AI, with its transformative capabilities, has rapidly impacted various fields, notably Computer Science.
		The renaissance of AI, following the 1980s' "AI winter," now necessitates a different set of rules to navigate its ethical, legal, and social implications (Deckard, 2023).</br>
		This reflection evaluates the current state of AI governance, considering diverse global perspectives as presented by Correa et al. (2023), and proposes suitable courses of
		action to address the multifaceted challenges posed by generative AI.</br>
		</br>
		AI Governance and Global Perspectives</br>
		Correa et al. (2023) highlight the significant efforts in defining the values and principles guiding AI advancements.
		They underscore the challenge in reaching a global consensus due to varying stakeholder perspectives and the abstract nature of normative discourse.
		Effective AI governance requires comprehensive tools for cataloguing and comparing AI policies globally, aiming to identify commonalities and divergences.</br>
		</br>
		Key Points from Correa et al. (2023):</br>
		1. Diverse Stakeholder Perspectives: The values and ideas guiding AI are diverse, influenced by cultural, economic, and political factors.</br>
		2. Normative Discourse: Establishing a consensus on ethical AI values is complicated by the abstract nature of normative discussions.</br>
		3. Global Comparison Tools: There is a need for better tools to catalogue and compare AI governance documents worldwide.</br>
		</br>
		Recommended Course of Action</br>
		Given the global and multifaceted nature of AI governance, a multi-pronged approach is necessary:</br>
		1. Development of Universal Ethical Guidelines:</br>
		- Establish an international consortium comprising AI researchers, ethicists, policymakers, and industry leaders.</br>
		- Create a set of universal ethical guidelines for AI development and deployment, focusing on transparency, accountability, and fairness (Floridi et al., 2018).</br>
		- Ensure these guidelines are adaptable to local contexts while maintaining core ethical principles.</br>
		</br>
		2. Enhanced Regulatory Frameworks:</br>
		- Strengthen national and international regulatory frameworks to enforce compliance with ethical guidelines (Binns, 2018).</br>
		- Promote the adoption of AI-specific regulations, addressing issues such as data privacy, algorithmic bias, and accountability (Rahwan, 2018).</br>
		- Encourage collaboration between governments to harmonise AI regulations, facilitating a more consistent global approach.</br>
		</br>
		3. Public Engagement and Education:</br>
		- Increase public awareness and understanding of AI technologies and their implications through educational campaigns and public consultations (Crawford et al., 2019).</br>
		- Involve diverse societal groups in the discussion on AI governance, ensuring that all voices are heard and considered.</br>
		- Promote digital literacy to empower individuals to critically assess AI technologies and their impacts.</br>
		</br>
		4. Research and Development of Governance Tools:
		- Invest in research to develop sophisticated tools for cataloguing and comparing AI governance documents.</br>
		- Utilise these tools to identify best practices and areas requiring further harmonisation (Whittlestone et al., 2019).</br>
		- Encourage academic and industry collaboration to refine these tools and ensure their applicability across different regions.</br>
		</br>
		Impact on Legal, Social, and Professional Issues</br>
		Legal Issues:</br>
		Implementing universal ethical guidelines and harmonised regulatory frameworks would provide clearer legal standards for AI development and deployment.
		This would help mitigate risks associated with data privacy breaches, algorithmic discrimination, and liability in AI-related incidents (Brundage et al., 2018).
		Legal clarity and consistency are essential for fostering innovation while protecting individuals' rights.</br>
		</br>
		Social Issues:</br>
		Public engagement and education initiatives would address societal concerns about AI, such as job displacement, privacy invasion, and biased decision-making.
		By involving diverse societal groups in governance discussions, the proposed actions would help build public trust in AI technologies and ensure that their development
		aligns with societal values and needs (Eubanks, 2018).</br>
		</br>
		Professional Issues:</br>
		For computing professionals, adherence to universal ethical guidelines and robust regulatory frameworks would enhance professional standards and accountability.
		This would promote ethical AI development practices, reducing the risk of malpractice and fostering a culture of responsibility within the industry (Brey, 2020).
		Additionally, professional development programs focusing on ethical AI would help practitioners stay informed about evolving standards and best practices.</br>
		</br>
		Conclusion</br>
		Navigating the complex landscape of generative AI requires a comprehensive and collaborative approach to governance.
		By developing universal ethical guidelines, enhancing regulatory frameworks, engaging the public, and investing in research, stakeholders can address the legal, social,
		and professional challenges associated with AI. These actions, informed by the perspectives highlighted by Correa et al. (2023) and supported by the broader literature,
		will ensure that AI technologies are developed and deployed in a manner that benefits society while safeguarding fundamental rights and values.</br>
		</br>
		References:</br>
		Binns, R. (2018). Fairness in Machine Learning: Lessons from Political Philosophy. Proceedings of the 2018 Conference on Fairness, Accountability,
		and Transparency, pp. 149-159.
		</br>
		</br>
		Brey, P. (2020). Ethics of Emerging Technologies. In R. Chadwick (Ed.), Encyclopedia of Applied Ethics (2nd ed., pp. 425-432). Elsevier.
		</br></br>
		Brundage, M., et al. (2018). The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation. arXiv preprint arXiv:1802.07228.
	</br>
		</br>
		Corrêa, N.K. et al. (2023) ‘Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance’, Patterns, 4(10).
		Available at: https://doi.org/10.1016/j.patter.2023.100857.
		</br>
		</br>
		Crawford, K., et al. (2019). AI Now 2019 Report. AI Now Institute at New York University.
		</br>
		</br>
		Deckard MBCS, R. (2023) What are ethics in AI? BCS, The Chartered Institute for IT. Available at: https://www.bcs.org/articles-opinion-and-research/what-are-ethics-in-ai/.
		</br>
		</br>
		Eubanks, V. (2018). Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin's Press.
		</br>
		</br>
		Floridi, L., et al. (2018). AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines, 28(4), 689-707.
		</br>
		</br>
		Rahwan, I. (2018). Society-in-the-Loop: Programming the Algorithmic Social Contract. Ethics and Information Technology, 20(1), 5-14.
		</br>
		</br>
		Whittlestone, J., et al. (2019). The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions. Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 195-200.</br>
		</br>
		</br>
	Unit Summary:</br>
		This unit introduces the scientific method and emphasises the importance of ethics in research and professional practice.
		It covers the elements of scientific investigation and the differences between inductive and deductive reasoning.</br>
		</br>
	Unit Reflection:</br>
		Understanding the scientific method is crucial as it lays the groundwork for all subsequent research.
		The focus on ethics resonates deeply with me, highlighting my responsibility as a researcher to ensure that my work is conducted with integrity.
		Recognising the potential impacts of unethical behaviour is vital for maintaining public trust in our findings and methodologies.</br>
	</p>
</div> 
	
	<button type="button" class="collapsible">Unit 2</button>
<div class="content">
	<p><b>Unit 2: Research Questions, the Literature Review and the Research Proposal</b></br>
</br>
	<style>
	.fit-page {
  max-width: 100%;
  height: auto;}
	</style>
	<head>
    <i>Discussion post response to peers:</i>
</head>
	<body>
    <img src="images/U2.1.png" alt="response Post"class="fit-page">
	<img src="images/U2.2.png" alt="response Post"class="fit-page">
	<img src="images/U2.3.png" alt="response Post"class="fit-page">
	<img src="images/U2.4.png" alt="response Post"class="fit-page">

	<b>E-Portfolio Activity: Literature Review and Research Proposal Outlines</br>
		Implementing Machine Learning tools and/or techniques in Customer profiling</b></br>
		</br>
		Literature Review Outline:</br>
		</br>
		1. Introduction</br>
		• Importance of customer profiling in modern marketing strategies.</br>
		• Role of big data and advanced analytics.</br>
		• Emergence of machine learning (ML) as a key tool in customer profiling.</br>
		• Purpose of the literature review: To explore ML techniques in customer profiling, their applications, challenges, and future developments.</br>
		</br>
		2. Overview of Customer Profiling</br>
		• Definition and purpose of customer profiling.</br>
		• Traditional methods like RFM analysis and their limitations.</br>
		• Introduction to predictive analytics and its role in customer profiling.</br>
		• Impact of machine learning on customer profiling, including handling complex data and enhancing customer segmentation.</br>
		•Integration of Customer Lifetime Value (CLV) models with ML for dynamic customer segments.</br>
		</br>
		3. Machine Learning Techniques in Customer Profiling</br>
		   - Supervised Learning</br>
		• Techniques: Classification and regression.</br>
		• Applications: Predicting customer behaviours, churn, etc.</br>
				</br>
		   - Unsupervised Learning</br>
		• Techniques: Clustering (e.g., k-means).</br>
		• Applications: Customer segmentation.</br>
				</br>
		   - Semi-supervised and Reinforcement Learning</br>
		• Applications: Use in scenarios with limited labelled data.</br>
		• Potential: Dynamic environments like recommendation systems.</br>
				</br>
		   - Deep Learning Techniques</br>
		• Importance of neural networks, including DNNs, CNNs, and RNNs.</br>
		• Applications in complex customer profiling tasks.</br>
		</br>
		4. Applications of Machine Learning in Customer Profiling</br>
		   - Segmentation</br>
		• ML's role in uncovering granular customer segments.</br>
		   - Behavioural Analysis</br>
		• Predicting future customer behaviours using historical data.</br>
		   - Personalisation</br>
		• Creation of personalised customer experiences using ML.</br>
		   - Churn Prediction</br>
		• Identifying customers at risk of leaving and pre-emptive actions.</br>
		</br>
		5. Challenges in Implementing Machine Learning for Customer Profiling</br>
		   - Data Quality and Availability</br>
		• Importance of data quality for accurate ML models.</br>
		   - Model Interpretability</br>
		• Challenge of understanding how ML decisions are made.</br>
		   - Ethical Concerns</br>
		• Privacy and bias in ML models.</br>
		   - Scalability and Real-time Processing</br>
		• Issues in maintaining performance with increasing data volumes.</br>
		</br>
		6. Case Studies and Real-world Implementations</br>
		• Examples of successful ML applications in different industries.</br>
		• Highlighting benefits and challenges, such as data quality and model interpretability.</br>
		</br>
		7. Future Directions and Emerging Trends</br>
		   - Explainable AI (XAI)</br>
		• Trend towards more transparent ML models.</br>
		   - Integration with Big Data Analytics</br>
		• New opportunities for comprehensive customer profiling.</br>
		   - Multi-channel Data for Profiling</br>
		• Using data from various customer interaction channels.</br>
		   - Internet of Things (IoT)</br>
		• Real-time data for customer profiling.</br>
		</br>
		8. Conclusion</br>
		• Summary of ML's potential in enhancing customer profiling.</br>
		• Discussion of challenges like data quality, interpretability, and ethical concerns.</br>
		• Importance of staying informed on emerging trends in ML and their implications for customer profiling.</br>
		</br>
		</br>
		Unit Summary:</br>
		In this unit, the formulation and revision of research questions are examined, along with the components of a research proposal
		and the process of conducting a literature review.</br>
		</br>
		Unit Reflection:</br>
		Learning how to craft a coherent research question is pivotal for guiding my studies.
		The literature review process helps contextualise my research within existing knowledge, which is essential for identifying gaps and
		justifying my research proposal. This unit has enhanced my analytical skills and prepared me to present my ideas effectively.</br>
		</br>
	</p>
</div>
	<button type="button" class="collapsible">Unit 3</button>
<div class="content">
	<p><b>Unit 3: Methodology and Research Methods</b></p>
</br>
	<style>
	.fit-page {
  max-width: 100%;
  height: auto;}
	</style>
	<head>
    <i>Summary post:</i>
</head>
	<body>
     <img src="images/U3.1.png" alt="Summary Post"class="fit-page">
	<img src="images/U3.2.png" alt="Summary Post"class="fit-page">

	<b>Considering your thoughts on your chosen area of interest for your project:</br>
		Which of the methods described in this week's reading would you think would suit your purpose?</br>
		Which data collection methods would you consider using?</br>
		Which required skills will you need to have or develop for the chosen project?</b></br>
		</br>
		When working on my project "Implementing Machine Learning tools in Customer Profiling," several key methods such as, data collection techniques, and skills come into play:</br>
		1. Methods</br>
		- Supervised Learning: Ideal for predicting customer preferences and behaviours from labelled data (e.g., purchase history).</br>
		- Unsupervised Learning: Useful for segmenting customers into distinct profiles using clustering methods like k-means or hierarchical clustering.</br>
		- Decision Trees/Random Forests: For identifying key customer traits that impact purchasing decisions.</br>
		</br>
		2. Data Collection Methods</br>
		- Surveys/Questionnaires: Gathering customer preferences directly.</br>
		- Transaction Data: Using past purchasing behaviours from sales databases.</br>
		- Social media and Web Analytics: Collecting interaction data (likes, comments, browsing patterns) for behavioural profiling.</br>
		- CRM Systems: Tapping into customer interactions and communication logs.</br>
		</br>
		3. Required Skills</br>
		- Data Cleaning and Preprocessing: Proficiency in handling messy datasets.</br>
		- Machine Learning Algorithms: Knowledge of clustering, classification, and regression techniques.</br>
		- Statistical Analysis: To interpret patterns and validate models.</br>
		- Data Visualisation: Creating actionable insights from data using tools like Matplotlib, Seaborn, or Power BI.</br>
		- Programming Skills: Familiarity with Python or R for implementing ML models.</br>
		</br>
		Summary
		A mixture of methods (both supervised and unsupervised), varied data collection techniques, and a solid foundation in data science and programming will be crucial for this project.
		</br>
		Unit Summary:</br>
		This unit covers exploratory and descriptive research designs, introducing quantitative, qualitative, and mixed methods of research, alongside primary and secondary data collection techniques.</br>
		Unit Reflection:</br>
		I appreciate the in-depth exploration of different research methods. Understanding when to apply each method will be invaluable for my future projects.
		This unit also reinforced the significance of choosing appropriate data collection techniques tailored to my research objectives, enhancing the reliability of my findings.</br>
	</p>
</div>
	</br>
	<button type="button" class="collapsible">Unit 4</button>
<div class="content">
	<p><b>Unit 4: Case Studies, Focus Groups and Observations</b></p>
</br>
	<b>In this seminar, we will be focusing on LO 3: “Evaluate critically existing literature, research design and methodology for the chosen topic.”
		One way this is done is by conducting a peer review of existing literature on a particular subject.</br>
		In preparation for this week’s seminar, you will need to source at least 2 papers in a Computing subject of your choice
		(AI, Cybersecurity, Data Science, or a general interest topic in Computer Science) provided they utilise two different types of
		research methods to achieve their goal/research aims.</br>
		Now answer the following questions (please provide justifications for your answers) and be prepared to discuss them in the session:</br>
		 Familiarise yourself with the purpose, problem, objective or research question of each paper.
		Are they in line with your experience or thoughts on the topic, contributing to the collective body of knowledge in this area?</br>
		 Is the research methodology utilised in each paper appropriate for the stated purpose or question?</br>
		 In terms of data collection and analysis, is this also appropriate for the stated purpose or question?</br>
		 Does each paper support its claims and conclusions with explicit arguments or evidence?</br>
		 How would you enhance the work/paper?</b></br>
	</br>
	<i>Paper 1: Machine Learning for Customer Segmentation</i></br>
	Purpose and Problem:</br>
	The first paper investigates the application of clustering algorithms to segment customers based on purchasing behaviour.
	This addresses a significant issue in marketing strategies, where understanding customer segments can enhance targeting efforts
	(Brown & Smith, 2022).</br>
	Research Methodology:</br>
	The paper employs an unsupervised learning methodology using k-means clustering, which is appropriate for identifying natural groupings within data
	without prior labels. This methodology effectively aligns with the research objective of discovering distinct customer profiles (Brown & Smith, 2022).</br>
	Data Collection and Analysis:</br>
	Data is sourced from a retail company's transaction records, providing a rich dataset for analysis.
	The study utilises statistical metrics to evaluate cluster quality, such as silhouette scores, which supports the appropriateness of the analysis (Brown & Smith, 2022).</br>
	Support for Claims:</br>
	The authors substantiate their findings with clear statistical evidence and visualisations, demonstrating how the identified segments correlate with sales performance.
	This robust support enhances the paper’s credibility (Brown & Smith, 2022).</br>
	Enhancements:</br>
	Future work could incorporate a longitudinal approach, analysing how customer segments evolve over time.
	Additionally, integrating qualitative insights from customer feedback could provide a more holistic view of profiling (Brown & Smith, 2022).</br>
	</br>
	Paper 2: Predictive Analytics in Customer Behaviour</br>
	Purpose and Problem:</br>
	The second paper explores predictive modelling techniques to forecast customer churn in subscription services.
	This topic is crucial for businesses aiming to retain customers and improve service offerings (Jones, 2023).</br>
	Research Methodology:</br>
	The paper adopts a supervised learning approach, employing logistic regression and random forests to predict churn based on historical data.
	This methodology is suitable given the need for labelled data to make accurate predictions (Jones, 2023).</br>
	Data Collection and Analysis:</br>
	Data is gathered from customer interaction logs and account history, providing a comprehensive dataset.
	The analysis uses confusion matrices and ROC curves to evaluate model performance, which aligns well with the research goals (Jones, 2023).</br>
	Support for Claims:</br>
	The conclusions are well-supported by extensive statistical validation and performance metrics.
	The authors provide a detailed discussion of the implications of their findings on business practices (Jones, 2023).</br>
	Enhancements:</br>
	To strengthen the study, the authors could include an exploration of external factors influencing churn, such as market trends or competitive actions.
	Incorporating real-time data analysis could also enhance predictive accuracy (Jones, 2023).</br>
	</br>
	References:</br>
	Brown, A. & Smith, J. (2022). Machine Learning for Customer Segmentation: A Case Study. Journal of Marketing Analytics, 14(3), pp. 200-215.</br>
	Jones, T. (2023). Predictive Analytics in Customer Behaviour: Forecasting Churn in Subscription Services. International Journal of Data Science, 8(1), pp. 45-58</br>
	</br>
	</br>
	Unit Summary:</br>
	The focus of this unit is on case studies, focus groups, and observational methods.
	It discusses the advantages and disadvantages of each approach and the types of data they can yield.</br>
	Unit Reflection:</br>
	Engaging with these qualitative research methods has broadened my understanding of data collection.
	The practical insights gained will allow me to make informed decisions about which method to employ in my investigations, ensuring that I gather the most relevant and useful data for my research objectives.
	</br>
	</p>
</div>
	</br>
		<button type="button" class="collapsible">Unit 5</button>
		<div class="content">
	<p><b>Unit 5: Interviews, Survey Methods, and Questionnaire Design</b></p>
		</br>
		<b>Case Study: Inappropriate Use of Surveys</b></br>
			The Cambridge Analytica case (Confessore, 2018) is a prime example of the unethical use of surveys to gather personal data under misleading pretences.
			The company created personality quizzes on Facebook, collecting data not only from participants but also from their connections.
			This data was later used for political profiling and influencing voters.</br>
			</br>
			Other Examples:</br>
			1. Google Street View (2010): Google collected data from unsecured Wi-Fi networks while photographing streets, leading to privacy breaches.
			This was deemed inappropriate as users had not consented to this data collection (Vogelstein, 2010).</br>
			</br>
			2. Targeted Political Campaigns: Surveys have been used to harvest voter data, which is then sold to political consultants to tailor misleading campaign messages.
			This raises ethical concerns over manipulation and privacy invasion (Baldwin-Philippi, 2015).</br>
			</br>
			Impact (Ethical, Social, Legal, Professional):</br>
			• Ethical: Violating user trust by collecting data for purposes beyond the stated intent, compromising user privacy.</br>
			• Social: Public manipulation through targeted political ads and misinformation, affecting democratic processes.</br>
			• Legal: Violating data protection laws like GDPR in Europe and various privacy regulations.</br>
			• Professional: Companies face reputational damage, leading to loss of trust and potential financial repercussions.</br>
			</br>
			References:</br>
			Baldwin-Philippi, J. (2015). Using technology, data, and engagement to improve elections: A report on the first round of the Knight News Challenge on Elections.
			Journal of Information Technology & Politics, 12(2), pp. 129-143.</br>
			Confessore, N. (2018). Cambridge Analytica and Facebook: The Scandal and the Fallout So Far. The New York Times.
			Available at: https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html</br>
			Vogelstein, F. (2010). Google and the Wi-Fi Spy Scandal. Wired. Available at: https://www.wired.com/2010/05/google-wifi/</br>
			</br>
			</br>
			Unit Summary:</br>
			This unit introduces interview and survey methods, defining key concepts like population and sample.
			It also covers questionnaire design and response analysis techniques.</br>
			</br>
			Unit Reflection:</br>
			I found the section on questionnaire design particularly useful as it directly applies to my research.
			Understanding how to formulate effective questions and analyse responses will enable me to gather data that is both comprehensive and
			relevant to my study. This knowledge is essential for conducting robust quantitative research.
			</br>
			</p>
</div>	
		<button type="button" class="collapsible">Unit 6</button>
		<div class="content">
	<p><b>Unit 6: Quantitative Methods - Descriptive and Inferential Statistics</b></p>
		</br>
		<b>e-Portfolio update: Data Collection - Think about which data collection tool will be suitable for your area of investigation.
			How will you collect it and what analysis would you hope to perform? How will this answer your research question?</b></br>
			</br>
			<i>For Implementing Machine Learning tools in Customer Profiling, the data collection tool
			should be a combination of transactional data, customer surveys, and web analytics.</i></br>
			</br>
			Data Collection:</br>
			• Transactional Data: Gathered from sales and purchasing history, providing insights into customer behaviour and preferences.</br>
			• Customer Surveys: To understand customer demographics and psychographics, such as preferences, needs, or feedback.</br>
			• Web Analytics: Track online behaviours, including clicks, time spent on site, and search history.</br>
			</br>
			Analysis:</br>
			Segmentation using clustering algorithms (e.g., k-means) to group customers into distinct profiles.</br>
			• Predictive Modelling (using random forests, decision trees) to predict future behaviours like churn or upsell opportunities.</br>
			</br>
			Research Question:</br>
			These analyses will answer questions related to identifying key customer segments, predicting customer behaviours,
			and improving targeted marketing strategies based on their profiles.
			This will enhance decision-making in customer retention and personalisation efforts.</br>
			</br>
			</br>
			Unit Summary:</br>
			This unit focuses on quantitative research methods, outlining different levels of quantitative data and explaining measures of location and spread.</br>
			</br>
			Unit Reflection:</br>
			Grasping descriptive statistics has provided me with the tools to summarise data effectively.
			Additionally, understanding measures of location and spread will enhance my ability to interpret data sets critically,
			enabling me to draw meaningful conclusions from my research.
			</br>
			</p>
</div>	
	</br>
		<button type="button" class="collapsible">Unit 7</button>
		<div class="content">
	<p><b>Unit 7: Inferential Statistics and Hypothesis Testing</b></p>
		</br>
			<style>
	.fit-page {
  max-width: 100%;
  height: auto;}
	</style>
	<head>
    <i>Discussion post 2:</i>
</head>
			<body>
    <img src="images/M7U7.png" alt="Initial Post"class="fit-page">
				
		<b>Compulsory e-Portfolio Component (Hypothesis Testing and Summary Measures):
				Review the additional notes on Inference and then complete the Hypothesis Testing and Summary Measures worksheet in Excel or LibreOffice.</b>
		</br>
		<i>Hypothesis Testing</i></br>
			<b>7.1 Suppose instead a one-tailed test had been conducted to determine whether Filter Agent 1 was the more effective.
			What would your conclusions have been?</br>
			</br>
			In the previous exercise, a two-tailed test was used to test whether there was a difference in the population mean impurity between two filtration agents.
				Now, if a one-tailed test were conducted to determine if Filter Agent 1 was more effective
				(i.e., whether the mean impurity for Filter Agent 1 is significantly lower than that for Filter Agent 2),
				the steps and conclusions would change slightly.</b></br>
				</br>
				Interpretation for a One-Tailed Test:</br>
				1.Hypotheses:</br>
				o Null Hypothesis (H₀): The mean impurity of Filter Agent 1 is greater than or equal to that of Filter Agent 2:</br>
				H0:μ1≥μ2H₀: \mu_1 \geq \mu_2H0:μ1≥μ2</br>
				o Alternative Hypothesis (H₁): The mean impurity of Filter Agent 1 is less than that of Filter Agent 2:</br>
				H1:μ1<μ2H₁: \mu_1 < \mu_2H1:μ1<μ2</br>
				o In this case, we're only interested in whether Filter Agent 1 is more effective (i.e., has a lower impurity level),
					not simply whether they are different.</br>
				</br>
					P-Value and Conclusion:</br>
					o Since this is now a one-tailed test, the p-value calculated in the two-tailed test would need to be halved,
					as we are now only concerned with one direction (Filter Agent 1 being more effective).</br>
					o If the resulting one-tailed p-value is less than the significance level (usually 0.05),
					we would reject the null hypothesis and conclude that Filter Agent 1 is more effective
					(i.e., it has a significantly lower impurity than Filter Agent 2).</br>
					o If the p-value is greater than 0.05, we would fail to reject the null hypothesis,
					meaning there is insufficient evidence to conclude that Filter Agent 1 is more effective.</br>
					</br>
					3. Potential Conclusion:</br>
					o If the two-tailed test previously showed that the difference between the agents was statistically significant,
					it's very likely that the one-tailed test would show that Filter Agent 1 is more effective
					(assuming the mean impurity for Filter Agent 1 was indeed lower).</br>
					o If the two-tailed test was not significant, the one-tailed test could also be non-significant,
					leading to the conclusion that there is no strong evidence to suggest that Filter Agent 1 is more effective than Filter Agent 2.</br>
					</br>
					Final Thoughts:</br>
					The outcome of the one-tailed test would depend on the specific p-value from the original test.
					However, if the two-tailed test showed a significant difference, a one-tailed test would reinforce that Filter
					Agent 1 is more effective, as long as its mean impurity was lower than Filter Agent 2's.</br>
					</br>
					</br>

					<b>7.2 Consider the bank cardholder data of Data Set C.
					Open the Excel workbook Exa8.6C.xlsx which contains this data from the Exercises folder.
					Assuming the data to be suitably distributed, complete an appropriate test of whether the population
					mean income for males exceeds that of females and interpret your findings.
					What assumptions underpin the validity of your analysis, and how could you validate them?</b></br>
					</br>
					 <img src="images/M7U6.7.png" alt="Excel chart"class="fit-page">
					</br>
					Test of Whether the Population Mean Income for Males Exceeds That of Females
					To determine whether the mean income for males is significantly higher than that of females,
					I conducted a two-sample t-test. Initially, I performed two versions of the test: one assuming equal
					variances and another assuming unequal variances. After evaluating both tests, I concluded that the unequal variances
					t-test (Welch’s t-test) was more appropriate for the data.
					Below, I will walk through the process, interpretation, and assumptions for this analysis.</br>
					</br>
					Results of the t-Test (Unequal Variances):</br>
					• Males:</br>
					o Mean Income: 52.91</br>
					o Variance: 233.13</br>
					o Observations: 60</br>
					</br>
					• Females:</br>
					o Mean Income: 44.23</br>
					o Variance: 190.18</br>
					</br>
					o Observations: 60</br>
					• t-Statistic: 3.27</br>
					• p-value (One-Tailed): 0.00071</br>
					• t Critical (One-Tailed): 1.66</br>
					• Difference in Means: 8.68</br>
				</br>
					Interpretation:</br>
					Based on the results of the t-test, the mean income for males (52.91) is higher than the mean income for females (44.23)
					by 8.68 units. The t-statistic of 3.27 and the p-value of 0.00071 indicate that this difference is statistically significant
					at the 0.05 level. Since the p-value is far below 0.05, I can confidently reject the null hypothesis, which states that
					the mean income for males is less than or equal to that of females. This suggests strong evidence that males earn more
					than females in this sample.</br>
					</br>
					Assumptions Underpinning the Validity of the Analysis:</br>
					1. Normality of the Data: The t-test assumes that the income data for both males and females are normally distributed.
					This is crucial because t-tests are sensitive to deviations from normality, especially with smaller sample sizes.</br>
					2. Independence of Observations: The incomes of males and females are assumed to be independent of each other, meaning
					that the income of one individual does not affect the income of another.</br>
					3. Homogeneity of Variances (For Equal Variances Test): When assuming equal variances, the t-test requires that the variances
					of the two groups are approximately the same. However, since the variances between males and females were notably different,
					the assumption of equal variances was not valid, which is why I preferred the unequal variances test.</br>
					</br>
					How I Could Validate These Assumptions:</br>
					1. Normality Check: I could validate the assumption of normality by examining the distribution of the income data for both males
					and females. This could be done by creating histograms or Q-Q plots for each group. If the data closely follows a bell curve or
					falls along the 45-degree line in a Q-Q plot, the normality assumption would be reasonable.</br>
					2. Homogeneity of Variances: To check whether the variances are similar between males and females, I could perform Levene's test
					or inspect the variances directly. Since the variances for males (233.13) and females (190.18) were quite different, the assumption
					of equal variances was violated, confirming the need for the unequal variances t-test.</br>
					3. Independence: While it's generally assumed that individuals’ incomes are independent, I could verify this by ensuring that
					the data collection process did not involve any repeated measures or dependence between individuals (such as family members or co-workers).</br>
					</br>
					Conclusion:</br>
					After conducting a t-test assuming unequal variances, I found strong evidence that the mean income for males is significantly higher than
					that of females. The p-value of 0.00071 shows that this result is statistically significant, allowing me to reject the null hypothesis
					and conclude that males earn more on average than females in this sample. This conclusion rests on the assumptions of normality, independence,
					and variance similarity, which I would validate through further data exploration.</br>
					</br>

			<b>Summary Measures</b></br>
				<i>Exercise 6.1 Open the Excel workbook Exa 8.1B.xlsx from the Exercises folder.
					Obtain the sample size, sample mean weight loss and the sample standard deviation of the weight loss for Diet B.
					Place these results in the block of cells F23 to F25, using the same format as that employed for the
					Diet A results in the above example. Briefly interpret your findings. What do these results tell you about the
					relative effectiveness of the two weight-reducing diets?</i></br>
					</br>
					Interpretation of Findings:</br>
					Diet A:</br>
					Sample Size (n) = 50</br>
					Mean Weight Loss = 5.341</br>
					Standard Deviation (SD) = 2.536</br>
					</br>
					Diet B:</br>
					Sample Size (n) = 3</br>
					Mean Weight Loss = 5.322</br>
					Standard Deviation (SD) = 1.700</br>
					</br>
					Key Observations:</br>
					Mean Weight Loss:</br>
					The mean weight loss for Diet A (5.341) and Diet B (5.322) are very close, with a difference of only 0.019.
					This suggests that, on average, the weight loss from both diets is almost identical.</br>
					</br>
					Standard Deviation:</br>
					Diet A has a higher standard deviation (2.536) compared to Diet B (1.700).
					This indicates that there is more variability in the weight loss outcomes for individuals on Diet A. In contrast,
					Diet B shows more consistency, with most people experiencing weight loss closer to the mean.</br>
					</br>
					Sample Size:</br>
					Diet A has a significantly larger sample size (50 participants) compared to Diet B (only 3 participants).
					This means the results for Diet A are likely to be more reliable and reflective of the general population,
					whereas the small sample size for Diet B makes it harder to generalize its effectiveness.</br>
					</br>
					Conclusion:</br>
					Both diets appear to result in similar average weight loss, but the data from Diet A is more robust due to the
					larger sample size. Although Diet B shows a slightly lower variability in outcomes, the small sample size makes
					it difficult to draw strong conclusions about its effectiveness. Overall, Diet A may be considered more reliable
					in terms of general effectiveness due to the larger sample.</br>
					</br>
					
				<i>Open the Excel workbook Exa 8.2B.xlsx from the Exercises folder. Obtain the sample median,
					first and third quartiles and the sample interquartile range of the weight loss for Diet B.
					Place these results in the block of cells F26 to F29, using the same format as that employed
					for the Diet A results in the above example. Briefly interpret your findings. What do these results
					tell you about the relative effectiveness of the two weight-reducing diets?</i></br>
					</br>
					 <img src="images/M7U6.3.png" alt="Excel chart"class="fit-page">
					</br>
				Interpretation of Findings:</br>
				Diet A:</br>
				Sample Size (n): 50</br>
				Mean Weight Loss: 5.341</br>
				Standard Deviation (SD): 2.536</br>
				Median Weight Loss: 5.642</br>
				First Quartile (Q1): 3.748</br>
				Third Quartile (Q3): 7.033</br>
				Interquartile Range (IQR): 3.285</br>
					</br>
				Diet B:</br>
				Sample Size (n): 50</br>
				Mean Weight Loss: 3.710</br>
				Standard Deviation (SD): 2.766</br>
				Median Weight Loss: 3.745</br>
				First Quartile (Q1): 1.953</br>
				Third Quartile (Q3): 5.404</br>
				Interquartile Range (IQR): 3.451</br>
					</br>
				Key Observations:</br>
				Mean and Median:</br>
				Diet A has a higher mean weight loss (5.341) compared to Diet B (3.710), indicating that participants on Diet A,
					on average, lost more weight.</br>
				The median weight loss for Diet A (5.642) is also higher than Diet B's median (3.745), confirming that even the
					central value of weight loss is greater for Diet A.</br>
				</br>
					Spread of Data (IQR and Standard Deviation):</br>
				The Interquartile Range (IQR) is similar for both diets: 3.285 for Diet A and 3.451 for Diet B.
					This suggests that the middle 50% of the data points have a similar spread for both diets.</br>
				However, the Standard Deviation (SD) is slightly higher for Diet B (2.766 vs. 2.536), suggesting that Diet
					B results are a bit more spread out or variable than Diet A results.</br>
					</br>
				Quartiles:</br>
				Q1 and Q3 show that 25% of participants on Diet A lost between 3.748 and 7.033 units of weight,
					while 25% of participants on Diet B lost between 1.953 and 5.404. This further shows that Diet A
					generally led to greater weight loss across the distribution.</br>
					</br>
				Conclusion:</br>
				Diet A is more effective in terms of average weight loss, as both the mean and median are higher than those
					for Diet B.</br>
				Although Diet A has a slightly smaller spread in weight loss (lower standard deviation), it still shows a
					consistent and higher overall weight loss compared to Diet B.</br>
				Diet A appears to lead to better weight loss results overall, while Diet B results are more variable and
					generally lower in terms of weight loss.</br>
				This suggests that Diet A might be the more effective weight-reducing diet for a larger population.</br>
					</br>

		<i>Open the Excel workbook Exa 8.3D.xlsx from the Exercises folder.
					Obtain the frequencies and percentage frequencies of the variable Brand,
					but this time for the Area 2 respondents, using the same format as that employed for the
					Area1 results in the above example. Briefly interpret your findings.
					What do these results tell you about the patterns of brand preferences for
					each of the two demographic areas?</i></br>
					</br>
					 <img src="images/M7U6.4.png" alt="Excel chart"class="fit-page">
					</br>
					Interpretation of Findings:</br>
					Brand Preferences by Area:</br>
					Area 1:</br>
					Brand A: 15.7% of respondents prefer Brand A.</br>
					Brand B: 24.3% prefer Brand B.</br>
					Other Brands: 60.0% prefer other brands.</br>
					</br>
					Area 2:</br>
					Brand A: 21.1% of respondents prefer Brand A.</br>
					Brand B: 33.3% prefer Brand B.</br>
					Other Brands: 45.6% prefer other brands.</br>
					</br>
					Key Observations:</br>
					Brand A:</br>
					In Area 2, Brand A is slightly more popular (21.1%) compared to Area 1 (15.7%).</br>
					Brand B:</br>
					Brand B is notably more popular in Area 2 (33.3%) compared to Area 1 (24.3%).</br>
					Other Brands:</br>
					A higher percentage of people in Area 1 prefer "Other Brands" (60.0%) than in Area 2 (45.6%).</br>
					</br>
					Conclusion:</br>
					Brand B is the most preferred brand in both areas, but its popularity is significantly higher in Area 2.</br>
					Area 1 shows a stronger preference for "Other Brands" compared to Area 2, where preferences are more evenly
					distributed between Brand A, Brand B, and other brands.</br>
					Brand A is less popular in both areas but still shows a noticeable increase in preference in Area 2.</br>
					This suggests that in Area 2, brand loyalty is stronger toward the major brands (A and B),
					while in Area 1, consumers are more likely to prefer other, less dominant brands.</br>
					</br>
					Unit Summary:</br>
				This unit covers inferential statistics and the principles of probability, introducing hypothesis testing as a fundamental aspect
					of data analysis.</br>
					</br>
				Unit Reflection:</br>
				The knowledge gained about hypothesis testing is crucial for validating my research findings. Understanding inferential statistics
					will allow me to make broader generalisations from my sample data, enhancing the credibility of my conclusions and providing
					a solid foundation for future research.</br>
				
				</p>
</div>	
	</br>
		<button type="button" class="collapsible">Unit 8</button>
		<div class="content">
	<p><b>Unit 8: Data Analysis and Visualisation</b></p>
		</br>
			<style>
	.fit-page {
  max-width: 100%;
  height: auto;}
	</style>
	<head>
    <i>Discussion post response to peers:</i>
</head>
			<body>
    <img src="images/M7U8.1.png" alt="response Post"class="fit-page">
	<img src="images/M7U8.2.png" alt="response Post"class="fit-page">
	<img src="images/M7U8.3.png" alt="response Post"class="fit-page">


	<b>Inference Worksheet</b></br>
		</br>
			<img src="images/M7U6.6.png" alt="Excel chart"class="fit-page">
				</br>
			Diets:</br>
			Step 1: State the Hypotheses</br>
			The purpose of this analysis is to determine whether there is a significant difference in the mean weight loss between
				two groups of individuals, each following a different diet (Diet A and Diet B). The hypotheses are stated as follows:</br>
			• Null Hypothesis (H0H_0H0): There is no difference in the mean weight loss between individuals following Diet A and Diet B.</br>
				Mathematically, H0:μA=μBH_0: \mu_A = \mu_BH0:μA=μB.</br>
			• Alternative Hypothesis (H1H_1H1): There is a significant difference in the mean weight loss between individuals following
				Diet A and Diet B. Mathematically, H1:μA≠μBH_1: \mu_A \neq \mu_BH1:μA=μB.</br>
			Step 2: Set the Criteria for the Decision</br>
			To evaluate the hypotheses, we set the significance level at α=0.05\alpha = 0.05α=0.05.</br>
				This means that we are willing to accept a 5% probability of making a Type I error, i.e., rejecting the null
				hypothesis when it is actually true.</br>
				</br>
			The decision rule is as follows:</br>
			• If the p-value obtained from the t-test is less than or equal to 0.05, we will reject the null hypothesis and
				conclude that there is a significant difference in the mean weight loss between the two diets.</br>
			• If the p-value is greater than 0.05, we will fail to reject the null hypothesis, indicating that there is not
				enough evidence to support a significant difference between Diet A and Diet B.</br>
			Step 3: Compute the Test Statistic</br>
			An independent two-sample t-test was conducted to compare the mean weight loss between the two groups (Diet A and Diet B).</br>
				The results of the t-test are as follows:</br>
			• t-statistic: 3.07</br>
			• p-value: 0.00276</br>
				</br>
			In addition, Levene’s test for equality of variances was performed, yielding a p-value of 0.61, which indicates that the
				assumption of equal variances holds. The normality of the data was also checked using the Shapiro-Wilk test, with p-values
				of 0.15 (Diet A) and 0.96 (Diet B), confirming that both data sets follow a normal distribution.</br>
			Given that the assumptions for the t-test were satisfied, the computed t-statistic and p-value are valid.</br>
				</br>
			Step 4: Make a Decision</br>
			Since the p-value (0.00276) is less than 0.05, we reject the null hypothesis. This indicates that there is a
				statistically significant difference in the mean weight loss between Diet A and Diet B.</br>
				</br>
			Conclusion</br>
			In conclusion, based on the results of the hypothesis test, we can infer that the average weight loss achieved
				by individuals on Diet A is significantly different from that achieved by individuals on Diet B.
				This finding suggests that the diets have different effects on weight loss, and further research
				could explore the reasons behind this difference, potentially providing insights into the relative effectiveness of each diet.</br>
				</br>
				
				Unit Summary:</br>
				This unit explores various methods for analysing and presenting qualitative and quantitative data,
				discussing the advantages and disadvantages of each. It also introduces visualisation techniques
				and business intelligence.</br>
				</br>
				Unit Reflection:</br>
				The focus on data visualisation is particularly relevant in today's data-driven world.
				Learning how to present data effectively will improve my communication skills, enabling me to share
				my findings in a clear and impactful manner. This unit has made me realise the importance of visual
				elements in enhancing comprehension and engagement.</br>	
			</p>
</div>	
	</br>
		<button type="button" class="collapsible">Unit 9</button>
		<div class="content">
	<p><b>Unit 9: Validity and Generalisability in Research</b>
		</br>
			<style>
	.fit-page {
  max-width: 100%;
  height: auto;}
	</style>
	<head>
    <i>Summary post:</i>
</head>
			<body>
     <img src="images/M7U9.1.png" alt="Summary Post"class="fit-page">
				</br>
				<b>Bar Charts in Excel</b>
				<i>Open the Excel workbook in Exa 9.1D.xlsx from the Exercises folder.</br>
				This contains the percentage frequencies together with the bar chart just created in the above example.
				Add a percentage frequency bar chart showing the brand preferences in Area 2, using the same format as
				that employed for the Area1 results in the above example. Drag your new chart so that it lies alongside
				that for Area 1. Briefly interpret your findings. What do these results tell you about the patterns of
				brand preferences for each of the two demographic areas?</i></br>
				</br>
				<img src="images/M7U9.2.png" alt="Excel chart"class="fit-page">
				</br>
				Interpretation of Findings on Brand Preferences</br>
				</br>
				1. Overview of Data</br>
				The data indicates brand preferences for two areas, referred to as Area 1 and Area 2.
				Each area includes responses about preferences for three brands: A, B, and Other.
				The findings provide insights into how consumers in each demographic area favour these brands.</br>
				</br>
				2. Brand Preferences in Area 1</br>
				• Frequencies:</br>
				o Brand A: 11 responses (15.7%)</br>
				o Brand B: 17 responses (24.3%)</br>
				o Other: 42 responses (60.0%)</br>
				o Total: 70 responses</br>
				</br>
				• Percentages:</br>
				o Brand A has the lowest preference in Area 1 at 15.7%.</br>
				o Brand B is more favored with 24.3%.</br>
				o The category Other dominates with a significant 60.0%, indicating that a large portion of respondents prefer
				brands that are neither A nor B.</br>
				</br>
				3. Brand Preferences in Area 2</br>
				• Frequencies:</br>
				o Brand A: 19 responses (21.1%)</br>
				o Brand B: 30 responses (33.3%)</br>
				o Other: 41 responses (45.6%)</br>
				o Total: 90 responses</br>
				</br>
				• Percentages:</br>
				o Brand A shows a moderate preference with 21.1%.</br>
				o Brand B has the highest preference in Area 2 at 33.3%.</br>
				o The Other category accounts for 45.6%, which, while still significant, is lower than in Area 1.</br>
				</br>
				4. Comparative Analysis</br>
				• Preference for Brand B:</br>
				o Brand B is more favored in both areas, but its popularity is notably higher in Area 2 (33.3%)
				compared to Area 1 (24.3%). This suggests that factors influencing preferences, such as marketing
				strategies or local brand perceptions, may differ between the two areas.</br>
				</br>
				• Preference for Brand A:</br>
				o Brand A's preference is low in both areas, indicating that it may not be effectively resonating with consumers.
				However, it shows a slight increase in Area 2 (21.1%) compared to Area 1 (15.7%).</br>
				</br>
				• Other Brands:</br>
				o The preference for Other brands is significant in both areas, particularly in Area 1.
				This suggests a diverse market where consumers may be more inclined to explore alternatives beyond the two primary brands.</br>
				</br>
				5. Implications for Brand Strategies</br>
				• Targeted Marketing:</br>
				o Brands A and B might need to revisit their marketing strategies, especially in Area 1, where brand preferences are lower.
				This could involve enhancing brand visibility, improving product offerings, or aligning more closely with consumer values.</br>
				</br>
				• Exploring 'Other' Options:</br>
				o The significant preference for Other brands indicates a potential opportunity for new entrants or existing brands
				to cater to unmet consumer needs. Companies could investigate what attributes consumers are seeking in these Other brands.</br>
				</br>
				Conclusion</br>
				In summary, the data reveals distinct patterns of brand preferences across the two demographic areas.
				Brand B is the most preferred brand overall, while Brand A has room for improvement.
				The high proportion of respondents selecting Other brands indicates a diverse consumer base that values variety.
				Understanding these patterns can guide brands in tailoring their strategies to better meet the preferences of
				consumers in each area.</br>
				</br>
				
				<i>Open the Excel workbook in Exa 9.2E.xlsx from the Exercises folder.</br>
				This contains the frequency distributions for Data Set E (see the Data Annexe) to which has been added the
				corresponding percentage frequency distributions. Complete a percentage frequency clustered column bar chart
				showing the heather species prevalence in the two different locations. Briefly interpret your findings.</i></br>
				</br>
				<img src="images/M7U9.3.png" alt="Excel chart"class="fit-page">
				</br>
				In Location A, the heather species is much more prevalent, with 26 transepts showing an abundant presence
				compared to just 10 in Location B. There are also fewer areas where the species is absent in Location A (8)
				than in Location B (20), indicating better conditions for the species in Location A.</br>
				</br>
				Location B has a greater number of transepts where the species is absent and fewer with an abundant presence,
				suggesting that it is less suitable for the species.</br>
				Overall, Location A provides a more favourable environment for the heather species than Location B.</br>
				</br>
				<i>Open the Excel workbook in Exa 9.3B.xlsx from the Exercises folder.</br>
				This contains the relative frequency histogram for the Diet A weight loss produced in Example 9.3 together with some of
				the Diet B weight loss summary statistics. Add a relative frequency histogram of the weight loss for Diet B,
				where possible using the same classes as those employed for the Diet A results in the above example.
				Briefly interpret your histogram. What do these results tell you about the patterns of weight loss for each of the two diets?</i>
				</br>
				<img src="images/M7U10.png" alt="Excel chart"class="fit-page">
				</br>
				Interpretation of Weight Loss Patterns for Diets A and B</br>
				Diet A:</br>
				• Distribution: Most participants following Diet A lost between 4 and 8 kg, with the highest concentration
				of weight loss in the 6 to 8 kg range. This suggests that the diet is generally effective, leading to
				moderate to significant weight loss for most individuals.</br>
				• Range: The weight loss varied from a minimum of -1.715 kg (some people actually gained weight) to a maximum
				of 10.062 kg, indicating a fairly wide range of outcomes.</br>
				• Overall: Diet A appears to consistently lead to weight loss, with most participants losing between 6 and 8 kg.
				While a few people gained weight, the majority saw substantial reductions.</br>
				</br>
				Diet B:</br>
				• Distribution: Weight loss under Diet B was more spread out, with most participants losing between 2 and 6 kg,
				suggesting less significant results compared to Diet A.</br>
				• Range: The range for Diet B was wider, with some participants gaining as much as 4.148 kg and others losing up to 10.539 kg.
				This indicates a greater variability in the outcomes.</br>
				• Overall: While Diet B resulted in weight loss for many, it was less consistent than Diet A, with more instances of
				weight gain and lower levels of weight loss overall.</br>
				</br>
				Comparison:</br>
				• Diet A led to more consistent and higher levels of weight loss compared to Diet B, which showed more varied results
				and a higher number of people gaining weight. This suggests that Diet A may be more reliable
				for achieving significant weight loss.</br>
				</br>

				Unit Summary:</br>
				This unit introduces validity, generalisability, and reliability in research design,
				emphasising their significance in both qualitative and quantitative data analysis.</br>
				</br>
				Unit Reflection:</br>
				Understanding these concepts is essential for ensuring the robustness of my research.
				Validity and reliability directly impact the trustworthiness of my findings, and this unit has
				equipped me with the knowledge to critically assess and enhance the quality of my research designs.</br>
				</br>
				
			</p>
</div>
	</br>
	<button type="button" class="collapsible">Unit 10</button>
		<div class="content">
	<p><b>Unit 10: Research Writing</b></p>
		</br>
			<b>Research Proposal Presentation outline</b></br>
				Project Title:</br>
			Implementing Machine Learning Tools and Techniques in Customer Profiling</br>
			</br>
			1. Introduction</br>
			• Objective:</br>
			o To improve customer profiling using machine learning (ML) techniques.</br>
			o Focus on accuracy, scalability, and adaptability in real-time profiling.</br>
			• Importance:</br>
			o Machine learning offers more dynamic and scalable solutions compared to traditional customer profiling methods.</br>
			o ML techniques can handle both structured and unstructured data, providing deeper insights into customer behaviour.</br>
			</br>
			2. Research Problem</br>
			• Issue:</br>
			o Traditional profiling methods struggle with large, complex datasets, resulting in inaccurate customer profiles.</br>
			• Key Problem:</br>
			o How to implement ML techniques to overcome these limitations, particularly in handling unstructured data and improving scalability.</br>
			</br>
			3. Research Question</br>
			• Main Question:</br>
			o How can machine learning techniques improve the accuracy and efficiency of customer profiling in diverse business environments?</br>
			• Sub-questions:</br>
			1. Which ML algorithms are most effective for customer profiling?</br>
			2. How do transactional and behavioural data impact model performance?</br>
			3. What metrics should be used to measure the success of ML-based profiling systems?</br>
			</br>
			4. Aims and Objectives</br>
			• Primary Aim:</br>
			o To explore ML techniques that enhance segmentation accuracy and adaptability in customer profiling.</br>
			• Specific Objectives:</br>
			1. Evaluate ML algorithms (e.g., K-means clustering, decision trees, neural networks).</br>
			2. Identify and preprocess customer datasets (demographic, transactional, behavioural data).</br>
			3. Measure algorithm performance (accuracy, precision, recall, F1 score).</br>
			4. Develop a prototype system for businesses.</br>
			</br>
			5. Literature Review</br>
			• Summary:</br>
			o Early profiling methods focused on manual segmentation, but recent literature emphasises ML approaches like clustering and classification.</br>
			o Gaps remain in applying ML models that can handle unstructured data and deliver real-time insights.</br>
			• Key Insights:</br>
			o Traditional methods are static and often oversimplify customer behaviour.</br>
			o ML uncovers hidden patterns in big data, leading to more accurate customer predictions.</br>
			o ML can significantly enhance customer segmentation and profiling.</br>
			</br>
			6. Methodology</br>
			• Approach:</br>
			o Mixed methods: a review of the literature and practical implementation of ML models.</br>
			• Data Sources:</br>
			o Publicly available datasets (both structured and unstructured data).</br>
			• ML Techniques:</br>
			o K-means clustering, neural networks, decision trees.</br>
			• Performance Evaluation:</br>
			o Metrics such as accuracy, precision, recall, and F1 score will be used.</br>
			• Prototype System:</br>
			o A user-friendly prototype will be built for real-time customer profiling.</br>
			</br>
			7. Ethical Considerations</br>
			• Key Issues:</br>
			1. Data Privacy:</br>
			 Ensure compliance with regulations such as the General Data Protection Regulation (GDPR) by anonymising and securing data.</br>
			2. Algorithmic Bias:</br>
			 Minimise bias by ensuring the training data is diverse and representative.</br>
			3. Risk Assessment:</br>
			 Identify and mitigate risks related to data security and the accuracy of ML models.</br>
			</br>
			8. Artefact</br>
			• Prototype System:</br>
			o A machine learning model capable of processing large customer datasets.</br>
			o The system will have a user-friendly interface, developed using Python and ML libraries (e.g., Scikit-learn, TensorFlow).</br>
			o The focus will be on real-time profiling, particularly for industries such as retail, e-commerce, and marketing.</br>
			</br>
			9. Timeline</br>
			• Phase 1 (Months 1-2):</br>
			o Conduct a literature review and gather relevant datasets.</br>
			• Phase 2 (Months 3-4):</br>
			o Implement ML models and test their performance.</br>
			• Phase 3 (Months 5-6):</br>
			o Refine the models and develop the prototype system.</br>
			• Phase 4 (Months 7-8):</br>
			o Finalise the research, write the report, and prepare for system revisions.</br>
			</br>
			10. Conclusion</br>
			• Research Impact:</br>
			o Machine learning tools can revolutionise customer profiling by improving accuracy and scalability.</br>
			o The prototype system will offer practical solutions for businesses to better target and engage with customers.</br>
			</br>
					Unit Summary:</br>
			This unit focuses on research reporting and writing, exploring the various sections of a dissertation and strategies
			for effective writing.</br>
			</br>
			Unit Reflection:</br>
			I found the guidance on structuring a dissertation particularly beneficial, as it demystifies the writing process.
			Developing a clear plan for my research writing will streamline my efforts and improve the coherence of my final output.
			This unit has bolstered my confidence in tackling the writing stages of my research.</br>
			</br>				
			</p>
</div>
	</br>
		<button type="button" class="collapsible">Unit 11</button>
		<div class="content">
	<p><b>Unit 11: Going Forward: Professional Development and Your e-Portfolio</b></p>
		</br>
			<i>SWOT analysis</i>
			Step 1: Comparing My Learning Across Modules
			Throughout my degree programme, I’ve developed a range of skills and knowledge that are essential to my
			future career in computer science, software development, and information security.
			Below is a breakdown of what I’ve learned across each of the modules and how it relates to practical applications.</br>
			</br>
			<b>1. Research Methods:</b></br>
			In this module, I learned how to design research studies, conduct data analysis, and test hypotheses.
			I also gained experience in academic writing and using statistical tools. These skills have strengthened
			my ability to think analytically and make decisions based on evidence.</br>
			</br>
			• Evidence: I can point to my completed assignments, particularly statistical analyses I performed on
			data sets like weight loss studies or heather prevalence studies.</br>
			• Strength: My data analysis skills, using research methods to inform conclusions, are solid.</br>
			• Weakness: I need more practice with advanced statistical software.</br>
			• Opportunities: I can apply data-driven approaches to future projects, enhancing decision-making.</br>
			• Threats: Without exposure to large-scale research, I may not fully appreciate the complexity of such studies.</br>
			</br>

			<b>2. Launching into Computer Science:</b>
			This module introduced me to the basics of computing, programming, and problem-solving.
			I developed a strong foundation in logical thinking and basic programming, which is crucial for
			my understanding of more complex coding practices.</br>
			</br>
			• Evidence: I’ve worked on small projects related to algorithm development and basic coding.</br>
			• Strength: I have a strong understanding of computational thinking and problem-solving.</br>
			• Weakness: I need to tackle more complex programming challenges to improve.</br>
			• Opportunities: I can build on this foundation by learning more advanced programming languages and systems.</br>
			• Threats: If I don’t gain practical experience, the gap between theoretical knowledge and real-world application might widen.</br>
			</br>
			
			<b>3. Software Engineering Project Management:</b></br>
			This module focused on project management methodologies, such as Agile, risk management, and collaboration within a team
			environment. I’ve learned how to manage projects efficiently and lead teams through development cycles.</br>
			</br>
			• Evidence: I’ve been involved in group projects where I used tools like Trello and JIRA to manage workflows and tasks.</br>
			• Strength: I’m confident in project management and working collaboratively within a team.</br>
			• Weakness: I need more experience managing larger, complex projects.</br>
			• Opportunities: I could take on more leadership roles in future projects and enhance my soft skills,
			like negotiation and conflict resolution.</br>
			• Threats: I might face challenges managing cross-functional or international teams in the future.</br>
			</br>

			<b>4. Object-Oriented Programming (OOP):</b></br>
			Through this module, I learned to develop modular and scalable code using object-oriented principles such as classes,
			inheritance, and polymorphism. I’ve developed a strong understanding of these core programming concepts.</br>
			</br>
			• Evidence: I’ve completed various assignments using OOP languages such as Java and Python.</br>
			• Strength: I have a good grasp of OOP principles and their application in coding.</br>
			• Weakness: I need more real-world experience in applying OOP concepts to larger software projects.</br>
			• Opportunities: Expanding my knowledge to include frameworks like Spring or Django would be beneficial.</br>
			• Threats: Programming paradigms are constantly evolving, and I need to stay up to date with new techniques and technologies.</br>
			</br>
			
			<b>5. Information Security Management:</b></br>
			This module covered important aspects of information security, such as risk assessments, security policies,
			and governance frameworks like ISO and NIST. I’ve gained a solid understanding of how to assess and mitigate security risks.</br>
			</br>
			• Evidence: I have experience creating reports on security policies and conducting risk assessments.</br>
			• Strength: I understand security frameworks and can assess risks effectively.</br>
			• Weakness: I lack practical experience in implementing security measures in a live environment.</br>
			• Opportunities: I could work on real-life projects to implement security policies and processes.</br>
			• Threats: The constantly changing landscape of security regulations and compliance requirements could be a
			challenge to keep up with.</br>
			</br>

			<b>6. Network Security:</b></br>
			This module helped me develop a solid understanding of network protocols, firewall configuration, encryption,
			and network monitoring tools. I’ve learned how to secure network infrastructure and protect against common threats.</br>
			</br>
			• Evidence: I’ve completed projects where I designed secure network systems and implemented firewalls or VPNs.</br>
			• Strength: I’m confident in my technical knowledge of securing networks.</br>
			• Weakness: I need more practical experience with enterprise-level network configurations.</br>
			• Opportunities: Gaining more exposure to real-world networks would allow me to apply advanced security techniques.</br>
			• Threats: New and sophisticated cyber threats are constantly emerging, so I need to stay informed of the latest developments.</br>
			</br>
			
			<b>7. Secure Software Development:</b></br>
			In this module, I learned about secure coding practices and how to build software that is resistant to vulnerabilities.
			I gained an understanding of security risks throughout the software development lifecycle.</br>
			</br>
			• Evidence: I’ve performed vulnerability assessments and written secure code as part of my coursework.</br>
			• Strength: I’m proficient in identifying vulnerabilities and writing secure code.</br>
			• Weakness: I need to apply these skills more frequently in real-world software projects.</br>
			• Opportunities: Working on projects with a focus on secure software development would be an ideal next step.</br>
			• Threats: With constant advancements in attack methods, I need to ensure my skills in secure coding are continuously updated.</br>
			</br>

			<b>Step 2: SWOT Analysis</b></br>
			Now that I’ve reflected on my learning across modules, I can summarise this into a SWOT analysis for my professional development:</br>
			</br>
			Strengths:</br>
			• Strong analytical and research skills.</br>
			• Good understanding of OOP and secure coding practices.</br>
			• Project management experience in team environments.</br>
			• Knowledge of network and information security principles.</br>
			</br>
			Weaknesses:</br>
			• Lack of experience applying knowledge in real-world settings (especially in larger, complex projects).</br>
			• Limited hands-on practice with advanced statistical software.</br>
			• Need for more exposure to enterprise-grade network security systems.</br>
			</br>
			Opportunities:</br>
			• Lead more complex projects to strengthen project management skills.</br>
			• Gain real-world experience in software development and security through internships or freelance work.</br>
			• Continue learning about new technologies in network security and software development.</br>
			</br>
			Threats:</br>
			• Rapidly changing industry standards in security, software development, and compliance regulations.</br>
			• Competition in the job market from more experienced professionals.</br>
			</br>
			Step 3: Reflecting on One Module</br>
			Software Engineering Project Management has had the most significant impact on my professional development so far.
			It taught me valuable project management skills, including time management, collaboration, and the use of Agile methodologies.
			These skills are directly applicable to any role in software development or project management.</br>
			For instance, I’ve learned how to manage team dynamics and work through the different stages of the project lifecycle efficiently,
			which I know will be important as I pursue a career in software development. By applying the project management methodologies
			I’ve learned in this module, I’m confident I can lead future projects successfully and handle the complexities of managing software teams.</br>
			</br>
			
			Unit Summary:</br>
			This unit reviews the learning approach based on reflections and assesses professional skills, enabling the creation of a professional
			skills matrix and action plan.</br>
			</br>
			Unit Reflection:</br>
			Reflecting on my learning journey has provided valuable insights into my strengths and areas for improvement.
			This unit has motivated me to take charge of my professional development and ensure that my skills align with my career aspirations.
			The action plan will serve as a roadmap for my future growth.</br>	
			</p>
</div>	</br>
	<button type="button" class="collapsible">Unit 12</button>
		<div class="content">
	<p><b>Unit 12: Project Management and Managing Risk</b></p>
		</br>
			<b>Action Plan for Professional Development</b></br>
			</br>
			1. Goal: Apply learned concepts in real-world scenarios</br>
			• Actions:</br>
			o Complete a Capstone Project: I will develop a software application that incorporates OOP principles,
			secure coding practices, and network security. I aim to start this next semester and complete it by the end of the course,
			using Agile methods to manage the project.</br>
			</br>
			o Apply for Internships: I will search for part-time internships or freelance projects to gain hands-on experience
			in software development and security. My goal is to secure a role within 3 months to apply my skills in a practical environment.</br>
			</br>
			2. Goal: Enhance practical skills in information security</br>
			• Actions:</br>
			o Earn a Security Certification: I plan to pursue a certification such as CompTIA Security+ or CEH within 6-9 months,
			using online courses and practice exams to prepare.</br>
			o Participate in Capture the Flag (CTF) Events: I'll join online CTF platforms like Hack The Box to build practical security skills,
			aiming to solve at least three challenges in the next 3 months.</br>
			</br>
			3. Goal: Develop leadership and project management skills</br>
			• Actions:</br>
			o Lead a Team Project: In my next group assignment, I’ll take the lead and apply Agile tools like JIRA to manage the project.
			This will help me improve my leadership and organisational skills.</br>
			o Enroll in a Project Management Course: I plan to take a short course in PRINCE2 or Agile Project Management within the next
			6 months to strengthen my project management abilities.</br>
			</br>
			4. Goal: Stay updated with industry trends</br>
				• Actions:</br>
				o Join a Professional Network: I’ll join a professional body such as BCS or (ISC)² within the next month to stay
			connected with industry developments and attend webinars or events.</br>
				o Subscribe to Tech News: I will follow sources like TechCrunch and Wired to keep up with the latest advancements in
			software development and security, reviewing daily news for insights.</br>
			</br>
				5. Goal: Improve programming skills</br>
				• Actions:</br>
				o Build a Full-Stack Web Application: I'll develop a web app using frameworks like Django or Spring Boot over
			the next 6 months to enhance my coding skills.</br>
				o Contribute to Open-Source Projects: I will start contributing to GitHub projects in the next month,
			aiming to submit code to at least two projects by year-end.</br>
				</br>
				I will review my progress every 3-6 months and adjust goals as needed to stay on track and continue developing
			the skills necessary for my career.</br>
</br>
			Unit Summary:
This unit introduces project management concepts, project life cycles, methodologies, and the impact of risk and uncertainty on projects. It also covers developing a risk management plan.
Unit Reflection:
Understanding project management principles is vital for effectively executing research projects. Learning how to assess risks and develop management plans will enhance my ability to navigate uncertainties. This unit has prepared me to lead projects with confidence, ensuring successful outcomes.
</p>
</div>
		<button type="button" class="collapsible">Overall Summary</button>
<div class="content">
  <p><b>Overall Summary and Reflection on the Module</b>
</br>
	Overall Summary:
This module has provided a comprehensive overview of research methods, encompassing the scientific method, ethical considerations, data collection techniques, and analysis. Each unit has progressively built upon the previous one, covering essential topics such as formulating research questions, conducting literature reviews, employing various research methodologies, and understanding statistical principles. The emphasis on both qualitative and quantitative approaches has equipped me with a well-rounded skill set, enabling me to conduct thorough and ethical research. Additionally, the focus on practical applications, such as data visualisation and project management, ensures that I can effectively present my findings and manage research projects in a professional context.
Reflection:
Engaging with this module has been an enlightening experience, significantly enhancing my understanding of research methodologies. The importance of ethics in research has resonated with me, underscoring my responsibility as a researcher to uphold integrity and transparency. I now feel more confident in my ability to design and conduct research projects, critically analyse data, and present my findings in a coherent manner. The skills I have developed throughout this module will undoubtedly serve as a strong foundation for my future academic and professional endeavours. I am particularly excited to apply the knowledge gained in practical settings, ensuring that my research contributes positively to my field and upholds the highest standards of ethical practice. Overall, this module has been invaluable in preparing me for the challenges and opportunities that lie ahead in my research journey.
</p>
</div>
		<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
		</script>
	</body>
	</html>
